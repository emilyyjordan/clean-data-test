{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content'\n",
      "/Users/emily/Desktop/research/AdNet\n"
     ]
    }
   ],
   "source": [
    "cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'explorelib_test' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone -b main https://github.com/emilyyjordan/explorelib_test.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content/explorelib_test'\n",
      "/Users/emily/Desktop/research/AdNet\n"
     ]
    }
   ],
   "source": [
    "cd /content/explorelib_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/MattChanTK/gym-maze.git\n",
      "  Cloning https://github.com/MattChanTK/gym-maze.git to /private/var/folders/v0/v5lvj9690t51ny6395nmfg2r0000gn/T/pip-req-build-au2b9ssk\n",
      "Requirement already satisfied, skipping upgrade: gym in /Users/emily/opt/anaconda3/lib/python3.8/site-packages (from gym-maze==0.4) (0.26.2)\n",
      "Requirement already satisfied, skipping upgrade: pygame in /Users/emily/opt/anaconda3/lib/python3.8/site-packages (from gym-maze==0.4) (2.5.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /Users/emily/opt/anaconda3/lib/python3.8/site-packages (from gym-maze==0.4) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle>=1.2.0 in /Users/emily/opt/anaconda3/lib/python3.8/site-packages (from gym->gym-maze==0.4) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: gym-notices>=0.0.4 in /Users/emily/opt/anaconda3/lib/python3.8/site-packages (from gym->gym-maze==0.4) (0.0.8)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=4.8.0; python_version < \"3.10\" in /Users/emily/opt/anaconda3/lib/python3.8/site-packages (from gym->gym-maze==0.4) (6.8.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /Users/emily/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.8.0; python_version < \"3.10\"->gym->gym-maze==0.4) (3.4.0)\n",
      "Building wheels for collected packages: gym-maze\n",
      "  Building wheel for gym-maze (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym-maze: filename=gym_maze-0.4-py3-none-any.whl size=14254 sha256=d1d2ef1b897d472c16f655bd2c630900d6d0ab9b919cd0a62a4a32ead048fbd9\n",
      "  Stored in directory: /private/var/folders/v0/v5lvj9690t51ny6395nmfg2r0000gn/T/pip-ephem-wheel-cache-6xunf7ge/wheels/c0/37/d4/f0a6f35e8f410ad1e28ac3a775cabb3c31905330fda0ab7a3a\n",
      "Successfully built gym-maze\n",
      "Installing collected packages: gym-maze\n",
      "  Attempting uninstall: gym-maze\n",
      "    Found existing installation: gym-maze 0.4\n",
      "    Uninstalling gym-maze-0.4:\n",
      "      Successfully uninstalled gym-maze-0.4\n",
      "Successfully installed gym-maze-0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade git+https://github.com/MattChanTK/gym-maze.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'explorationlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6acad3bb9181>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mexplorationlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'explorationlib'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import csv\n",
    "import explorationlib\n",
    "\n",
    "\n",
    "from explorationlib.local_gym import BanditUniform4\n",
    "from explorationlib.local_gym import BanditChange4\n",
    "from explorationlib.local_gym import BanditUniform3\n",
    "from explorationlib.local_gym import BanditAddictive2\n",
    "from explorationlib.local_gym import BanditAnti2\n",
    "from explorationlib.local_gym import BanditNeutral\n",
    "from explorationlib.local_gym import BanditChange3\n",
    "from explorationlib.agent import BanditActorCritic\n",
    "from explorationlib.agent import Critic\n",
    "from explorationlib.agent import CriticUCB\n",
    "from explorationlib.agent import CriticNovelty\n",
    "from explorationlib.agent import EpsilonActor\n",
    "from explorationlib.agent import RandomActor\n",
    "from explorationlib.agent import SequentialActor\n",
    "from explorationlib.agent import SoftmaxActor\n",
    "from explorationlib.agent import BoundedRandomActor\n",
    "from explorationlib.agent import BoundedSequentialActor\n",
    "from explorationlib.agent import DeterministicActor\n",
    "from explorationlib.agent import BanditAdNet\n",
    "\n",
    "from explorationlib.run import experiment\n",
    "from explorationlib.score import total_reward\n",
    "from explorationlib.score import action_entropy\n",
    "from explorationlib.util import select_exp\n",
    "from explorationlib.util import load\n",
    "from explorationlib.util import save\n",
    "\n",
    "from explorationlib.plot import plot_bandit\n",
    "from explorationlib.plot import plot_bandit_actions\n",
    "from explorationlib.plot import plot_bandit_critic\n",
    "from explorationlib.plot import plot_bandit_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount GDrive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Addictive vs Neutral\n",
    "# Shared env params\n",
    "num_experiments = 1\n",
    "\n",
    "# plot env before\n",
    "envAddictive = BanditAddictive2()\n",
    "envAddictive.seed(seed = 42)\n",
    "seed = 42\n",
    "envAddictive.plotDecks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adn = BanditAdNet(\n",
    "    SoftmaxActor(num_actions=envAddictive.num_arms, beta=0.25),\n",
    "    Critic(num_inputs=envAddictive.num_arms, default_value=0.0),\n",
    "    lr_pos=1.0, lr_neg=0.1\n",
    ")\n",
    "\n",
    "# -\n",
    "agents = [adn]\n",
    "names = [\"adnet\"]\n",
    "colors = [\"yellow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running experiment with addictive environment\n",
    "num_steps = 100\n",
    "\n",
    "addictive_results = []\n",
    "for name, agent in zip(names, agents):\n",
    "    log = experiment(\n",
    "        f\"{name}\",\n",
    "        agent,\n",
    "        envAddictive,\n",
    "        num_steps=num_steps,\n",
    "        num_experiments=1,\n",
    "        dump=False,\n",
    "        split_state=False,\n",
    "    )\n",
    "    addictive_results.append(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize total rewards of addictive feedback schedule\n",
    "\n",
    "# Score\n",
    "scores = []\n",
    "for name, res, color in zip(names, addictive_results, colors):\n",
    "    r = total_reward(res)\n",
    "    scores.append(r)\n",
    "\n",
    "# Tabulate\n",
    "m, sd = [], []\n",
    "for (name, s, c) in zip(names, scores, colors):\n",
    "    m.append(np.mean(s))\n",
    "    sd.append(np.std(s))\n",
    "\n",
    "# Plot means\n",
    "fig = plt.figure(figsize=(3, 6))\n",
    "plt.bar(names, m, yerr=sd, color=colors, alpha=0.8)\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.xlabel(\"Agent\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def agent_df(agmax, almax, astep, given_beta, given_gamma):\n",
    "    df_columns = np.array(['Alpha Gain', 'Alpha Loss', 'Beta', 'Gamma', 'propA', 'propC'])\n",
    "    df = pd.DataFrame(dtype=float, columns=df_columns)\n",
    "\n",
    "    for cycle in np.arange(0, 250, 1):\n",
    "\n",
    "\n",
    "        #alpha gains should only be positive, 0 to 1 for example\n",
    "        for lr_pos in np.arange(0, agmax, astep):\n",
    "\n",
    "            #alpha loss should only be positive, 0 to 1 for example\n",
    "            for lr_neg in np.arange(0, almax, astep):\n",
    "\n",
    "                print(cycle)\n",
    "\n",
    "                beta = given_beta\n",
    "                gamma = given_gamma\n",
    "                lr_pos, lr_neg, beta = np.round(lr_pos, 3), np.round(lr_neg, 3), np.round(beta, 3)\n",
    "                envAddictive = BanditAddictive2()\n",
    "                agent = BanditAdNet(SoftmaxActor(num_actions=envAddictive.num_arms, beta= beta),\n",
    "                                  Critic(num_inputs=envAddictive.num_arms, default_value=0.0),\n",
    "                                  lr_pos = lr_pos, lr_neg = lr_neg)\n",
    "                data = log = experiment(\"envAddictive\",\n",
    "                                        agent,\n",
    "                                        envAddictive,\n",
    "                                        num_steps=100,\n",
    "                                        num_experiments=1,\n",
    "                                        dump=False,\n",
    "                                        split_state=False,\n",
    "                                        )\n",
    "                propA = envAddictive.deck_counters[0]/100.0\n",
    "                propC = envAddictive.deck_counters[1]/100.0\n",
    "\n",
    "                trial_df = pd.DataFrame([[lr_pos, lr_neg, beta, gamma, propA, propC]], columns = df_columns)\n",
    "                df = df.append(trial_df)\n",
    "                df.reset_index(drop=True, inplace=True)\n",
    "                clear_output()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf5 = agent_df(0.5, 0.5, .025, .5, 1) #changed first 2 agmax/almax from 1 to 0.5 and step from 0.05 to 0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Q-Agent data for Parameter analysis in a List of Q-Agent Dataframes\n",
    "mydfs_dup = [mydf5]\n",
    "mydfs = []\n",
    "\n",
    "for df in mydfs_dup:\n",
    "    df = df.groupby(['Alpha Gain', 'Alpha Loss'])[['Beta', 'Gamma','propA', 'propC']].mean().reset_index()\n",
    "    mydfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "\n",
    "grid_kws = {\"hspace\": 0, \"wspace\": 0.2}\n",
    "fig, axs = plt.subplots(1,1, figsize=(7,7), gridspec_kw=grid_kws)\n",
    "\n",
    "df = mydfs[0]\n",
    "\n",
    "print(\"heatmap values\")\n",
    "display(df)\n",
    "\n",
    "# saving the dataframe\n",
    "df.to_csv('/content/drive/My Drive/senior honors thesis/AddictiveHeatmap.csv', index=False)\n",
    "\n",
    "\n",
    "heatmap_dfA = pd.pivot(df, index = \"Alpha Loss\", columns = \"Alpha Gain\", values = \"propA\").astype('float')\n",
    "heatmapA = sns.heatmap(heatmap_dfA, square = True, cmap = 'hot', vmin=0, vmax=0.5, xticklabels=4, yticklabels = 4, cbar=True, ax= axs).invert_yaxis() #cmap = 'RdBu_r'\n",
    "beta = round(df[\"Beta\"][0], 3)\n",
    "gamma = round(df[\"Gamma\"][0], 3)\n",
    "fig.suptitle(f\"\\u03B2={beta}, \\u03B3={gamma} \\n\", fontsize = 16)\n",
    "axs.set(ylabel='$\\u03B1_{Loss}$')\n",
    "axs.set(xlabel='$\\u03B1_{Gain}$')\n",
    "axs.set_title(\"Proportion of Draws from Addictive Deck\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
